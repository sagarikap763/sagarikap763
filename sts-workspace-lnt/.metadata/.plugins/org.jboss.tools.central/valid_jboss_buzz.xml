<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>What's new in fabric8 Kubernetes client version 5.5.0</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XAHWbFVnMnA/whats-new-fabric8-kubernetes-client-version-550" /><author><name>Rohan Kumar</name></author><id>dec57063-7521-42ee-8d17-651322721a72</id><updated>2021-07-16T07:00:00Z</updated><published>2021-07-16T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://github.com/fabric8io/kubernetes-client"&gt;fabric8 Kubernetes client&lt;/a&gt; has been simplifying Java developers' use of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; for several years. Although the parent &lt;a href="https://fabric8.io/"&gt;fabric8 project&lt;/a&gt; has &lt;a href="https://developers.redhat.com/blog/2020/01/28/introduction-to-eclipse-jkube-java-tooling-for-kubernetes-and-red-hat-openshift"&gt;ended&lt;/a&gt;, the Kubernetes client continues to be popular, and the recent 5.5.0 release includes many new features and bug fixes.&lt;/p&gt; &lt;p&gt;This article takes a look at new features in fabric8 Kubernetes client, focusing on:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Dynamic clients&lt;/li&gt; &lt;li&gt;Dynamic informers&lt;/li&gt; &lt;li&gt;TLS certificate management&lt;/li&gt; &lt;li&gt;HTTP retry options&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; resources in the &lt;a href="https://docs.openshift.com/container-platform/3.7/dev_guide/openshift_pipeline.html#pipeline-openshift-dsl"&gt;OpenShift client DSL&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Knowing about these changes will help you avoid problems when you upgrade to the latest version of fabric8's &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; client for Kubernetes or Red Hat OpenShift.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The fabric8 development team mostly consists of Java developers, so the client's design is heavily influenced by a Java developer's perspective. I will demonstrate just a few of fabric8's features for using Kubernetes APIs in a Java environment.&lt;/p&gt; &lt;h2&gt;How to get the new fabric8 Java client&lt;/h2&gt; &lt;p&gt;You can find the most current fabric8 Java client release on &lt;a href="https://search.maven.org/artifact/io.fabric8/kubernetes-client/5.5.0/jar"&gt;Maven Central&lt;/a&gt;. To start using the new client, add it as a dependency in your Maven &lt;code&gt;pom.xml&lt;/code&gt; file. For Kubernetes, the dependency is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.fabric8&lt;/groupId&gt; &lt;artifactId&gt;kubernetes-client&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For OpenShift, it's:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.fabric8&lt;/groupId&gt; &lt;artifactId&gt;openshift-client&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;New features in fabric8 Kubernetes client 5.5.0&lt;/h2&gt; &lt;p&gt;In many areas, we've been watching developments in Kubernetes and OpenShift and listening to the needs of developers. I will cover the most important new features in the following sections.&lt;/p&gt; &lt;h3&gt;Dynamic clients: Unstructured and GenericKubernetesResource&lt;/h3&gt; &lt;p&gt;The Kubernetes Go language client has the concept of a &lt;em&gt;dynamic&lt;/em&gt; client and a generic Kubernetes type called &lt;code&gt;Unstructured&lt;/code&gt;. The fabric8 Kubernetes client already provided support for &lt;code&gt;Unstructured&lt;/code&gt; in its &lt;code&gt;CustomResource&lt;/code&gt; API using raw &lt;code&gt;HashMap&lt;/code&gt;s. In 5.5.0, we added a new type, &lt;code&gt;GenericKubernetesResource&lt;/code&gt;, which you can use for deserializing unknown types. Here is an example of using it to get a list of &lt;code&gt;CronTab&lt;/code&gt; resources (mentioned in the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition"&gt;Kubernetes CustomResourceDefinition guide&lt;/a&gt;) in a specified namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;try (KubernetesClient client = new DefaultKubernetesClient()) { CustomResourceDefinitionContext context = new CustomResourceDefinitionContext.Builder() .withVersion("v1") .withGroup("stable.example.com") .withScope("Namespaced") .withPlural("crontabs") .build(); System.out.println("CronTab resources in default namespace: "); client.genericKubernetesResources(context) .inNamespace("default") .list().getItems().stream() .map(GenericKubernetesResource::getMetadata) .map(ObjectMeta::getName) .forEach(System.out::println); } catch (KubernetesClientException e) { System.out.println("Exception received: " + e.getMessage()); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the future, we will provide additional methods to make using &lt;code&gt;GenericKubernetesResource&lt;/code&gt; even easier. We also plan to add support for applying the list of Kubernetes resources (right now the class works only for primitive Kubernetes resource lists), which can also contain custom resources&lt;/p&gt; &lt;h3&gt;Dynamic informers&lt;/h3&gt; &lt;p&gt;With the introduction of &lt;code&gt;GenericKubernetesResource&lt;/code&gt;, you can use the &lt;a href="https://javadoc.io/static/io.fabric8/kubernetes-client/4.6.1/io/fabric8/kubernetes/client/informers/SharedInformer.html"&gt;SharedInformer API&lt;/a&gt; for a &lt;code&gt;CustomResource&lt;/code&gt; without providing any type. The earlier &lt;code&gt;KubernetesClient&lt;/code&gt; raw API was missing support for &lt;code&gt;SharedInformer&lt;/code&gt;s, but now you can use informers from both &lt;code&gt;SharedInformerFactory&lt;/code&gt; and the DSL &lt;code&gt;inform()&lt;/code&gt; method.&lt;/p&gt; &lt;p&gt;Here is an example of using dynamic informers from &lt;code&gt;SharedInformerFactory&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;try (KubernetesClient client = new DefaultKubernetesClient()) { SharedInformerFactory informerFactory = client.informers(); CustomResourceDefinitionContext context = new CustomResourceDefinitionContext.Builder() .withGroup("stable.example.com") .withVersion("v1") .withPlural("crontabs") .withScope("Namespaced") .build(); SharedIndexInformer&lt;GenericKubernetesResource&gt; informer = informerFactory.sharedIndexInformerForCustomResource(context, 60 * 1000L); informer.addEventHandler(new ResourceEventHandler&lt;&gt;() { @Override public void onAdd(GenericKubernetesResource genericKubernetesResource) { System.out.printf("ADD %s\n", genericKubernetesResource.getMetadata().getName()); } @Override public void onUpdate(GenericKubernetesResource genericKubernetesResource, GenericKubernetesResource t1) { System.out.printf("UPDATE %s\n", genericKubernetesResource.getMetadata().getName()); } @Override public void onDelete(GenericKubernetesResource genericKubernetesResource, boolean b) { System.out.printf("DELETE %s\n", genericKubernetesResource.getMetadata().getName()); } }); informerFactory.startAllRegisteredInformers(); TimeUnit.MINUTES.sleep(10); } catch (InterruptedException e) { Thread.currentThread().interrupt(); e.printStackTrace(); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We’ve also added a new method to the DSL:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;inform(ResourceEventHandle eventHandler)&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This method can be used from within the DSL without having to create a &lt;code&gt;SharedInformerFactory&lt;/code&gt;. A &lt;code&gt;SharedIndexInformer&lt;/code&gt; will be started automatically, so there is no need to invoke the informer's &lt;code&gt;run()&lt;/code&gt; method. Here is an example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package io.fabric8.demos; import io.fabric8.kubernetes.api.model.Pod; import io.fabric8.kubernetes.client.DefaultKubernetesClient; import io.fabric8.kubernetes.client.KubernetesClient; import io.fabric8.kubernetes.client.informers.ResourceEventHandler; import java.util.concurrent.TimeUnit; public class DSLInformMethod { public static void main(String[] args) { try (KubernetesClient client = new DefaultKubernetesClient()) { client.pods().inNamespace("default").inform(new ResourceEventHandler&lt;&gt;() { @Override public void onAdd(Pod pod) { System.out.println(pod.getMetadata().getName() + " ADD "); } @Override public void onUpdate(Pod pod, Pod t1) { System.out.println(pod.getMetadata().getName() + " UPDATE "); } @Override public void onDelete(Pod pod, boolean b) { System.out.println(pod.getMetadata().getName() + " DELETE "); } }); TimeUnit.SECONDS.sleep(10 * 1000); } catch (InterruptedException interruptedException) { Thread.currentThread().interrupt(); interruptedException.printStackTrace(); } } } &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Certification management&lt;/h3&gt; &lt;p&gt;A new extension was contributed to this release: The &lt;a href="https://github.com/jetstack/cert-manager"&gt;JetStack&lt;/a&gt; &lt;code&gt;cert-manager&lt;/code&gt; extension. With this extension, you can manage TLS web certificates through &lt;code&gt;cert-manager&lt;/code&gt;-based CRDs from the Kubernetes API server in Java, using the &lt;code&gt;cert-manager&lt;/code&gt; extension.&lt;/p&gt; &lt;p&gt;Add this dependency to use the extension in your projects:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.fabric8&lt;/groupId&gt; &lt;artifactId&gt;certmanager-client&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a simple example of a &lt;code&gt;CerificateRequest&lt;/code&gt; using the extension:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package io.fabric8.demo; import io.fabric8.certmanager.api.model.meta.v1.ObjectReferenceBuilder; import io.fabric8.certmanager.api.model.v1.CertificateRequest; import io.fabric8.certmanager.api.model.v1.CertificateRequestBuilder; import io.fabric8.certmanager.client.CertManagerClient; import io.fabric8.certmanager.client.DefaultCertManagerClient; import io.fabric8.kubernetes.api.model.Duration; import java.text.ParseException; public class CertificateRequestExample { public static void main(String[] args) { try (CertManagerClient certManagerClient = new DefaultCertManagerClient()) { CertificateRequest certificateRequest = new CertificateRequestBuilder() .withNewMetadata().withName("my-ca-cr").endMetadata() .withNewSpec() .withRequest("base64encodedcert=") .withIsCA(false) .addToUsages("signing", "digital signature", "server auth") .withDuration(Duration.parse("90d")) .withIssuerRef(new ObjectReferenceBuilder() .withName("ca-issuer") .withKind("Issuer") .withGroup("cert-manager.io") .build()) .endSpec() .build(); certManagerClient.v1().certificateRequests().inNamespace("default").create(certificateRequest); } catch (ParseException e) { e.printStackTrace(); } } } &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;HTTP operation retry configuration options with exponential backoff&lt;/h3&gt; &lt;p&gt;According to &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#http-status-code"&gt;Kubernetes API conventions&lt;/a&gt;, when the HTTP status code is 500 (Status Internal Server Error), 503 (Status Service Unavailable), or 504 (Status Server Timeout), the suggested client recovery behavior is "retry with exponential backoff." In this release, we introduced additional configuration properties for this retry mechanism, shown in Table 1.&lt;/p&gt; &lt;div&gt; &lt;table&gt;&lt;caption&gt;Table 1: New properties to control HTTP retries in fabric8 Kubernetes client 5.5.0.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt; &lt;p&gt;&lt;strong&gt;Property name&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;th&gt; &lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;kubernetes.request.retry.backoffLimit&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Number of retry attempts on status codes &gt;= 500. Defaults to 0.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;kubernetes.request.retry.backoffInterval&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Retry initial backoff interval, in milliseconds. Defaults to 1000.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;h3&gt;OpenShift client DSL improvements&lt;/h3&gt; &lt;p&gt;The previous release provided support for all Kubernetes resources, as described in &lt;a href="https://github.com/fabric8io/kubernetes-client/issues/2912"&gt;this issue&lt;/a&gt; in the fabric8 Kubernetes client GitHub repository. We also had good coverage for OpenShift 3.x resources. But in OpenShift 4, newer resources were introduced by the installation of additional operators. Now we’re covering most of the resources offered by OpenShift 4. You can find more details in &lt;a href="https://github.com/fabric8io/kubernetes-client/issues/2949"&gt;this GitHub issue&lt;/a&gt;. Table 2 shows the newly added DSL methods.&lt;/p&gt; &lt;div&gt; &lt;table&gt;&lt;caption&gt;Table 2: New resource DSLs in fabric8 Kubernetes client 5.5.0&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt; &lt;p&gt;&lt;strong&gt;OpenShift resource&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;th&gt; &lt;p&gt;&lt;strong&gt;OpenShift client DSL&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 ClusterRole&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.clusterRoles()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 ResourceAccessReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.resourceAccessReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 LocalSubjectAccessReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.localSubjectAccessReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 LocalResourceAccessReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.localResourceAccessReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 SubjectRulesReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.subjectRulesReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 SelfSubjectRulesReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.selfSubjectRulesReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 RoleBindingRestrictions&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.roleBindingRestrictions()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;autoscaling.openshift.io/v1 ClusterAutoscaler&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.clusterAutoscaling().v1().clusterAutoscalers()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;autoscaling.openshift.io/v1beta1 MachineAutoscaler&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.clusterAutoscaling().v1beta1().machineAutoscalers()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;cloudcredential.openshift.io/v1 CredentialsRequest&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.credentialsRequests()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;config.openshift.io/v1 Authentication&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.config().authentications()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;config.openshift.io/v1 Console&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.config().consoles()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;config.openshift.io/v1 DNS&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.config().dnses()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;config.openshift.io/v1 Network&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.config().networks()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;console.openshift.io/v1alpha1 ConsolePlugin&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.console().consolePlugins()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;console.openshift.io/v1 ConsoleQuickStart&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.console().consoleQuickStarts()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;controlplane.operator.openshift.io/v1alpha1 PodNetworkConnectivityCheck&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operator().podNetworkConnectivityChecks()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;helm.openshift.io/v1beta1 HelmChartRepository&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.helmChartRepositories()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;image.openshift.io/v1 ImageSignature&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.imageSignatures()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;image.openshift.io/v1 ImageStreamImage&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.imageStreamImages()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;image.openshift.io/v1 ImageStreamImport&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.imageStreamImports()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;image.openshift.io/v1 ImageStreamMapping&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.imageStreamMappings()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machine.openshift.io/v1beta1 MachineHealthCheck&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machine().machineHealthChecks()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machine.openshift.io/v1beta1 MachineSet&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machine().machineSets()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machineconfiguration.openshift.io/v1 ContainerRuntimeConfig&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machineConfigurations().containerRuntimeConfigs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machineconfiguration.openshift.io/v1 ControllerConfig&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machineConfigurations().controllerConfigs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machineconfiguration.openshift.io/v1 KubeletConfig&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machineConfigurations().kubeletConfigs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machineconfiguration.openshift.io/v1 MachineConfigPool&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machineConfigurations().machineConfigPools()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machineconfiguration.openshift.io/v1 MachineConfig&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machineConfigurations().machineConfigs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;metal3.io/v1alpha1 BareMetalHost&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.bareMetalHosts()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;monitoring.coreos.com/v1alpha1 AlertmanagerConfig&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.monitoring().alertmanagerConfigs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;monitoring.coreos.com/v1 Probe&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.monitoring().probes()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;monitoring.coreos.com/v1 ThanosRuler&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.monitoring().thanosRulers()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;network.openshift.io/v1 HostSubnet&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.hostSubnets()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;network.operator.openshift.io/v1 OperatorPKI&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operatorPKIs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;oauth.openshift.io/v1 OAuthClientAuthorization&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.oAuthClientAuthorizations()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;oauth.openshift.io/v1 UserOAuthAccessToken&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.userOAuthAccessTokens()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;operator.openshift.io/v1 CloudCredential&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operator().cloudCredentials()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;operator.openshift.io/v1 ClusterCSIDriver&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operator().clusterCSIDrivers()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;operator.openshift.io/v1 Storage&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operator().storages()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;operators.coreos.com/v1 OperatorCondition&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operatorHub().operatorConditions()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;operators.coreos.com/v1 Operator&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operatorHub().operators()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;packages.operators.coreos.com/v1 PackageManifest&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operatorHub().packageManifests()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;security.openshift.io/v1 PodSecurityPolicyReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.podSecurityPolicyReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;security.openshift.io/v1 PodSecurityPolicySelfSubjectReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.podSecurityPolicySelfSubjectReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;security.openshift.io/v1 PodSecurityPolicySubjectReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.podSecurityPolicySubjectReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;template.openshift.io/v1 BrokerTemplateInstance&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.brokerTemplateInstances()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;template.openshift.io/v1 TemplateInstance&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.templateInstances()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;tuned.openshift.io/v1 Profile&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.tuned().profiles()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;tuned.openshift.io/v1 Tuned&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.tuned().tuneds()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;user.openshift.io/v1 Identity&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.identities()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;user.openshift.io/v1 UserIdentityMapping&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.userIdentityMappings()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;h3&gt;Other improvements&lt;/h3&gt; &lt;p&gt;Other notable improvements to the Kubernetes client in this release include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;fabric8 Knative extension: Knative model updated to v0.23.0.&lt;/li&gt; &lt;li&gt;fabric8 Tekton extension: Tekton pipeline model updated to v0.24.1.&lt;/li&gt; &lt;li&gt;fabric8 Tekton extension: Tekton triggers model updated to v0.13.0.&lt;/li&gt; &lt;li&gt;fabric8 Kubernetes mock server: Bug fixes related to ignoring the local kubeconfig, CRUD-mode fixes such as status subresource handling, apiVersion awareness, etc.&lt;/li&gt; &lt;li&gt;Introduction of &lt;code&gt;withNewFilter()&lt;/code&gt; in the &lt;code&gt;KubernetesClient&lt;/code&gt; DSL, offering better options for Kubernetes resource filtering.&lt;/li&gt; &lt;/ul&gt;&lt;h2 id="learn_more_about_fabric8-h2"&gt;Learn more about fabric8&lt;/h2&gt; &lt;p&gt;This article has demonstrated just a few of fabric8's features for using Kubernetes APIs in a Java environment. For more examples, see the &lt;a href="https://github.com/fabric8io/kubernetes-client/tree/master/kubernetes-examples/src/main/java/io/fabric8/kubernetes/examples"&gt;Kubernetes Java client examples repository&lt;/a&gt;. For a deep dive into using fabric8, visit the &lt;em&gt;&lt;a href="https://github.com/fabric8io/kubernetes-client/blob/master/doc/CHEATSHEET.md"&gt;fabric8 Kubernetes Java Client Cheat Sheet&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/16/whats-new-fabric8-kubernetes-client-version-550" title="What's new in fabric8 Kubernetes client version 5.5.0"&gt;What's new in fabric8 Kubernetes client version 5.5.0&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XAHWbFVnMnA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Rohan Kumar</dc:creator><dc:date>2021-07-16T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/16/whats-new-fabric8-kubernetes-client-version-550</feedburner:origLink></entry><entry><title type="html">Quarkus Newsletter #10</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/OxWpae7K68I/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-newsletter-10/</id><updated>2021-07-16T00:00:00Z</updated><content type="html">Ok… it’s been a while since our last newsletter but we’ve been busy working on a new format. We’re moved to an email subscription model starting with Issue #10. Our goal is to allow folks to sign up and get the cream of the crop articles delivered to their inbox...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/OxWpae7K68I" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-newsletter-10/</feedburner:origLink></entry><entry><title type="html">Cross-repo Pull Requests? build-chain tool to the rescue!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/c630UfdbrvU/cross-repo-pull-requests-build-chain-tool-to-the-rescue.html" /><author><name>Enrique Mingorance Cano</name></author><id>https://blog.kie.org/2021/07/cross-repo-pull-requests-build-chain-tool-to-the-rescue.html</id><updated>2021-07-15T14:43:34Z</updated><content type="html">Do you often need to change many repositories at once to implement a new feature? Do you need to create multiple Pull Requests in many repositories and need to build every repository on the correct branch to test it? Then you have the same problem we had several months ago at the KIE organization here at Red Hat. Now thanks to the we are already able to easily set up our GitHub Actions Workflows to build cross-repo Pull Requests for many different repositories. In this blog post, we’ll cover the problem surrounding cross-repo Pull Requests along with the steps to configure your first build-chain GitHub Actions Workflow. We’ll also present real-world examples and some additional cool things you can do with build-chain! Let’s get started. THE CROSS-REPO PULL REQUESTS PROBLEM Let’s suppose we have this repository dependency tree: Image 1: Drools’ repository dependency tree This is already a complex-enough repository dependency tree and some interesting scenarios can happen during development. But before we break down these scenarios and explain what we should be doing on each of them to verify that a PR is good and does not break anything, let’s define some important concepts. First of all, if you’re building a repository that depends on the development versions of other repositories, you should be building these repositories. These dependency-repositories are called UPSTREAM repositories. Also, if the repository you’re building is one of those repositories and there are repositories depending on the development version of your repository, you should be building those repositories as well. These repositories that depend on yours are called DOWNSTREAM repositories. Lastly, let’s call the repository you’re building the CURRENT repository. So to summarize, the build order should always be: UPSTREAM → CURRENT → DOWNSTREAM Now let’s imagine these different scenarios: 1. New PR on drools * CURRENT: drools * UPSTREAM: none * DOWNSTREAM: all others 2. New PR on optaplanner * CURRENT: optaplanner * UPSTREAM: drools * DOWNSTREAM: optaweb-employee-rostering, optaweb-vehicle-routing, and droolsjbpm-integration * Note: Since jbpm and kie-jpmml-integration are dependencies of DOWNSTREAM repositories, they are also considered UPSTREAM. 3. New PR on optaweb-vehicle-routing * CURRENT: optaweb-vehicle-routing * UPSTREAM: drools and optaplanner * DOWNSTREAM: none * Note: All the other projects are completely unrelated to this change and can be ignored. 4. New cross-repo PRs on drools and jbpm * CURRENT: drools and jbpm * UPSTREAM: none * DOWNSTREAM: all others * Note: This is a special case where we have two CURRENT repositories. In practice, this will result in two separate checks, where we have drools as CURRENT on one check, and jbpm as CURRENT and drools as UPSTREAM on another. As developers, we are only concerned with the effects of our changes in CURRENT and DOWNSTREAM projects, since we always assume that UPSTREAM projects are working fine. Since we depend on development versions of our UPSTREAM projects, it can be that a recent change in combination with our change breaks something, but that should appear on the tests as well, so nothing too concerning here. Now that you understand the problem, let’s see what can be done with GitHub Actions Workflows alone. SOLVING IT WITH PLAIN GITHUB ACTIONS WORKFLOWS The easiest and simplest way to solve it would be to create a new GitHub Action Workflow for every repository we want to cover. Something like: on: [pull_request] jobs: build: runs-on: ubuntu-latest strategy: matrix: projects: ["drools", "jbpm", "kie-jpmml-integration", "optaplanner", "droolsjbpm-integration", "optaweb-employee-rostering", "optaweb-vehicle-routing"] steps: - name: "Checkout ${{ matrix.projects }}" uses: actions/checkout@v2 with: repository: kiegroup/${{ matrix.projects }} ref: # Somehow we get the branch to checkout from env variables - name: "Setup Java for ${{ matrix.projects }}" uses: actions/setup-java@v1 with: java-version: 1.8 - name: "Execute Maven for ${{ matrix.projects }}" run: # mvn whatever goals and profiles... So we copy-paste this file into the .github/workflows directory for every project. Let’s make a couple of observations about this strategy and see the problems we have with it: * First of all, to get the checkout information about which branch to get (forked/not forked, pull request or not, how to relate pull requests with each other, how to get different target branches from each repository…). * We’re assuming that every repository can be built with Maven. What if there are different commands for each repository? More or less the same problem for every build system/technology we need to use. * The dependency tree is flattened into an array of projects. What if the order changes? * The information about building and the list of repositories is copy and pasted on every copy of the Workflow file. What if you have dozens of repositories? * Every project is being built every time, but we already saw on 3) that some repositories can be ignored during the checks. How do we avoid building unnecessary repositories? * There is no separation between UPSTREAM, CURRENT, and DOWNSTREAM projects. What if we want to build each category in a specific way? For example, skipping tests on UPSTREAM repositories can improve the performance a lot! * And so on… In the end, you will have too many Workflow files on too many repositories and you will have to maintain all of them. I assure you this is an ideal scenario if you want to go crazy! Let’s see how the build-chain helps us overcome those issues and minimize the number of files we have to maintain. THE BUILD-CHAIN WAY WHAT IS A BUILD-CHAIN? build-chain is an NPM package that allows running commands, no matter the command or technology behind them, for different repositories from GitHub in a single GitHub Action or CLI command. This is especially useful whenever you want to build interdependent repositories.  It solves the problem listed above where you have the need for cross-repo PRs with a complex repository dependency tree. The most important parts of the build-chain are the dependency-tree.yml and the definitions.yml files. They tell build-chain WHAT we want to build, and HOW, respectively. GITHUB ACTIONS WORKFLOWS CONFIGURATION To start using build-chain, you need to first define the structure of your repositories dependency tree. This is done on the dependency-tree.yml file. Using the dependency tree on Image 1, let’s create the dependency-tree.yml file: version: "2.0" dependencies: - project: kiegroup/drools - project: kiegroup/jbpm dependencies: - project: kiegroup/drools - project: kiegroup/optaplanner dependencies: - project: kiegroup/drools - project: kiegroup/kie-jpmml-integration dependencies: - project: kiegroup/jbpm - project: kiegroup/droolsjbpm-integration dependencies: - project: kiegroup/optaplanner - project: kiegroup/drools - project: kiegroup/jbpm - project: kiegroup/optaweb-employee-rostering dependencies: - project: kiegroup/optaplanner - project: kiegroup/optaweb-vehicle-routing dependencies: - project: kiegroup/optaplanner Now that we have that in place, we need to create our definitions.yml file, where we’ll configure HOW our repositories should be build depending on the position that they occupy on the build-chain (UPSTREAM, CURRENT, or DOWNSTREAM). version: "2.0" dependencies: ./dependency-tree.yaml default: # Define the default configuration for every repository. build-command: # We want to skip tests for every upstream repository to speed things up. upstream: mvn clean install -DskipTests # We want to execute this command for every project triggering the GitHub Action Workflow. Since there’s no default downstream configuration, current is used for DOWNSTREAM repositories. current: mvn clean install # Remove the UPSTREAM repositories to save disk space. after: upstream: rm -rf ./* build: # Additionally, we can define specific configuration per repository - project: kiegroup/drools build-command: # When drools is an UPSTREAM repository, this command will be used. upstream: mvn clean install -DskipTests -Psuper-fast-build # When drools is the CURRENT repository, this command will be used. current: mvn clean install -Pintegration-tests Since at the time of writing this post there’s no way to configure a Workflow for a group of repositories (Github does not allow it), we need to copy-paste the Workflow definition file in every repository that is part of the build-chain dependency-tree. Notice that it is not mandatory to do that, but if you open a PR on a project that doesn’t have this Workflow, the build-chain will not be triggered. As this is a file that’s going to be replicated throughout the many repositories you have, we have to keep it very minimal and make sure that it is the exact same on every repository. This is important for maintaining sanity while maintaining the build-chain. So here it is: on: [pull_request] jobs: build-chain: runs-on: ubuntu-latest steps: - name: "Set up JDK" uses: actions/setup-java@v1 with: java-version: 1.8 - name: "Run build-chain" id: build-chain uses: kiegroup/github-action-build-chain@v2.5 with: definition-file: https://whateverserver_url/definitions.yaml Pretty simple, isn’t it? Now for every pull_request event that occurs in your repository, the build-chain Action will be triggered and only the necessary repositories will be built. Also, you’re able to customize HOW these repositories are going to be built depending on the position they occupy during the build. This flexibility saves a lot of time when you need to alter your build-chain and prevents manual errors from occurring on specific long Workflows. Also, everything is centralized in one place so that if the dependency tree changes, you only have to alter one place, and if the commands for building a specific repository changes, you also only need to change one place! Now let’s see how we use build-chain on our real-world projects at the KIE organization. USING BUILD-CHAIN ON KIE REPOSITORIES Now that you’re aware of what the problem build-chain solves and the advantages it has, you can see what a real-world use case looks like. In the KIE organization, we have two major streams — Business Central and Kogito. Each has its own particular needs, but they share the repositories dependency tree, which makes the build-chain even more valuable. Below you’ll find details about each. BUSINESS CENTRAL STREAM PROJECT DEPENDENCIES We have two different project dependencies files. * The one for community projects:   * The one for prod projects (prod + community), where you can see the extension from community projects:   FLOW DEFINITIONS We have different kinds of flows for every build type we want to cover. * Compilation:  *   * Full downstream flow:   * Pull request flow:   * Upstream flow:   * Productization flow:   GITHUB ACTION FLOWS * Pull request:   * Upstream:   The rest of the flow definitions are used from CLI to allow developers to test their new developments locally or from any other automation system (remember the build chain tool is an NPM tool, not only for GitHub actions). KOGITO STREAM We have two interesting cases here. The first one is the kogito projects themselves and the other one is the drools PRs flow to test drools is not breaking anything from kogito projects. KOGITO We need to build the kogito chain for every project PR. We additionally have the requirement to check kogito-runtimes changes don’t break anything from the downstream projects. Image 2: Kogito’s Repository dependency tree PROJECT DEPENDENCIES FLOW DEFINITION GITHUB ACTION FLOWS We have a PR flow per repository (kogito-runtimes, apps, and examples). We additionally check the kogito-runtimes changes don’t break the rest of the project from the hierarchy, the flows are basically the same, just the starting-project input decides which project is the one triggering the job. * kogito-runtimes:   * PRs: * kogito-apps checking:   * kogito-examples checking:   * optaplanner checking:   * kogito-apps PRs:    * kogito-examples PRs: DROOLS We have the requirement to assure drools’ changes are not breaking anything from kogito, since kogito depends on drools artifacts.   Image 3: Drools + RHBA repository dependency tree PROJECT DEPENDENCIES  It’s very interesting to see how the project dependency extends the one from RHBA, this way drools or kogito should not be worried about the RHBA hierarchy. They just say, kogito-runtimes depends on drools (which is not even declared on kogito-project-dependencies.yaml file but in the one from RHBA).   FLOW DEFINITION It’s interesting to see in order . GITHUB ACTION FLOWS Both flows are basically (apart from OS running the job) the same, just the starting-project input is different since we want to start the process from the top leaf of the tree * Pull request flow: * Full downstream flow:   ADDITIONAL GOOD STUFF Besides all the advantages listed above, build-chain also provides handy additional features to help you maintain your Workflows and even speed up development. REPOSITORIES DEPENDENCY TREE IMAGE Using the same definition file you use to build your repositories, and thanks to the , it is possible to automatically generate an image of your repositories dependency tree. You can do it both locally and as a step on your GitHub Actions Workflows. jobs: build-chain-files-generator: runs-on: ubuntu-latest name: File Generation steps: - name: "build-chain repository dependency tree image generation" uses: kiegroup/build-chain-files-generator@main with: definition-file: https://whateverserver_url/definition.yaml file-type: image output-file-path: ./docs/project-dependencies-hierarchy.png or build-chain-files-generator -df https://whateverserver_url/definition.yaml -o image.png image RUNNING THE BUILD-CHAIN LOCALLY By executing a CLI command to locally test it (replacing $PROJECT and $ID) build-chain-action -df https://whateverserver_url/definition.yaml build pr -url https://github.com/kiegroup/$PROJECT/pull/$ID RUNNING THE TOOL FROM THE AUTOMATION SYSTEM (DIFFERENT TO GITHUB ACTIONS) Since it is possible to run it from CLI it is obviously possible to run it from any kind of automation system like Jenkins. In our case at RedHat we create as many Jenkins jobs as different build-chain flows we have, just adding this step to the pipeline, the project tree and definition will be reused and there’s only one single place to maintain different build systems. stage('Build projects') { steps { script { def buildChainActionInfo = isFDBP() ? [action: 'fd', file: 'downstream-production-config.yaml'] : isFDB() ? [action: 'fd', file: 'full-downstream-config.yaml'] : isPR() ? [action: 'pr', file: 'pull-request-config.yaml'] : isCompile() ? [action: 'fd', file: 'compilation-config.yaml'] : [action: 'pr', file: 'upstream-config.yaml'] def SETTINGS_XML_ID = isFDBP() ? '5d9884a1-178a-4d67-a3ac-9735d2df2cef' : '771ff52a-a8b4-40e6-9b22-d54c7314aa1e' configFileProvider([configFile(fileId: SETTINGS_XML_ID, variable: 'MAVEN_SETTINGS_FILE')]) { withCredentials([string(credentialsId: 'kie-ci1-token', variable: 'GITHUB_TOKEN')]) { sh "build-chain-action -token=${GITHUB_TOKEN} -df='https://raw.githubusercontent.com/${GROUP}/droolsjbpm-build-bootstrap/${BRANCH}/.ci/${buildChainActionInfo.file}' -folder='bc' build ${buildChainActionInfo.action} -url=${env.ghprbPullLink} --skipParallelCheckout -cct '(^mvn .*)||$1 -s ${MAVEN_SETTINGS_FILE} -Dmaven.wagon.http.ssl.insecure=true'" } } } } } You can check the whole pipeline from   NEXT STEPS AND LIMITATIONS DISK SPACE LIMITATION We have cases where the projects at the bottom of the tree (see image2) (like ) require to check out and build more than 20 repositories, (at the time the post was written) which is not enough in some cases. Fortunately, we found the way to release 49GB up to 63GB free by executing this step   Additionally, we can remove the target folder after the project is built thanks to the after section from the build chain definition file like we do on: Image 4: RHBA dependency tree. CONCLUSION SUMMARY We have been using this tool for and repositories for a year and we can say it’s a very useful tool which solves the cross-repo pull requests problem on every build system we have. After a year of experience with the tool we can say the tool offers: * The chance to have the same GitHub flow file for every repository involved on the chain. * The chance to have the same Jenkins pipeline (or any other build system) for every repository involved on the chain. * One single point to maintain for every build system. * The chance to easily test your changes locally thanks to the CLI. * Different commands per project based on the tree hierarchy and project triggering the job. * Decides which branch to take based on project tree information, project triggering the job, PR’s target and source information, and open PRs. USEFUL LINKS * [Build chain tool] * [Build chain npm package]   * [Configuration reader] * [File generator] * [RHBA definition and project tree files]   * [RHBA flows] I would like to thanks Tiago Bento for pushing this post creation and reviewing it. Thanks for reading. Featured photo by The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/c630UfdbrvU" height="1" width="1" alt=""/&gt;</content><dc:creator>Enrique Mingorance Cano</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/cross-repo-pull-requests-build-chain-tool-to-the-rescue.html</feedburner:origLink></entry><entry><title>Troubleshooting application performance with Red Hat OpenShift metrics, Part 2: The test environment</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/THpX2R8G2-U/troubleshooting-application-performance-red-hat-openshift-metrics-part-2-test" /><author><name>Pavel Macik</name></author><id>7435fb0d-5726-4ffa-8190-4571e2248d49</id><updated>2021-07-15T07:00:00Z</updated><published>2021-07-15T07:00:00Z</published><summary type="html">&lt;p&gt;This series shows how I solved a real-life performance problem by gathering metrics from &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. Part 1 laid out the &lt;a href="https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1"&gt;development environment and requirements&lt;/a&gt;. Now, in Part 2, we will set up the test environment and I will introduce two different test scenarios.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please read &lt;a href="https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1"&gt;&lt;em&gt;Troubleshooting application performance with Red Hat OpenShift metrics, Part 1: Requirements&lt;/em&gt;&lt;/a&gt; for an overview of the project and test requirements.&lt;/p&gt; &lt;h2&gt;Provisioning the OpenShift cluster&lt;/h2&gt; &lt;p&gt;The Developer Sandbox for Red Hat OpenShift provides a temporary cloud platform, which is useful for testing an application before deploying it. As I explained in Part 1, the application under test is the &lt;a href="https://developers.redhat.com/blog/2019/12/19/introducing-the-service-binding-operator"&gt;Service Binding Operator&lt;/a&gt;. I ran the projection using &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;, but you could use any OpenShift cluster to generate metrics. I used the &lt;a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/" target="_blank"&gt;openshift-install&lt;/a&gt; tool to set up a cluster in Amazon Web Services (AWS) for a &lt;a href="https://github.com/codeready-toolchain/toolchain-e2e/blob/master/setup/README.adoc#prereqs" target="_blank"&gt;sandbox that meets the prerequisites of an operator in production&lt;/a&gt;. The provisions for the cluster were:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Three coordinator nodes of m5.4xlarge size (16 vCPU, 64 GiB memory)&lt;/li&gt; &lt;li&gt;Three worker nodes of m5.2xlarge size (8 vCPU, 32 GiB memory)&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Installing the Developer Sandbox&lt;/h2&gt; &lt;p&gt;My installation uses the Developer Sandbox &lt;a href="https://github.com/codeready-toolchain/toolchain-e2e/blob/master/setup/README.adoc" target="_blank"&gt;setup tool&lt;/a&gt;, which I introduced in Part 1. The installation steps are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Clone the &lt;a href="https://github.com/codeready-toolchain/toolchain-e2e"&gt;repository containing the CodeReady Toolchain E2E tests&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone git@github.com:codeready-toolchain/toolchain-e2e.git&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Use a Makefile to install the Developer Sandbox Operators:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;make dev-deploy-e2e&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Installing the Service Binding Operator&lt;/h2&gt; &lt;p&gt;Use the following installation script to install the Service Binding Operator and Red Hat OpenShift Application Services Operator in OpenShift Container Platform:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;export SBO_INDEX_IMAGE=${SBO_INDEX_IMAGE:-quay.io/redhat-developer/servicebinding-operator:index} export SBO_CHANNEL=${SBO_CHANNEL:-beta} export SBO_PACKAGE=${SBO_PACKAGE:-service-binding-operator} export SBO_CATSRC_NAMESPACE=${SBO_CATSRC_NAMESPACE:-openshift-marketplace} export SBO_CATSRC_NAME=${SBO_CATSRC_NAME:-sbo-operators} export RHOAS_INDEX_IMAGE=${RHOAS_INDEX_IMAGE:-quay.io/rhoas/service-operator-registry:autolatest} export RHOAS_CHANNEL=${RHOAS_CHANNEL:-beta} export RHOAS_PACKAGE=${RHOAS_PACKAGE:-rhoas-operator} export RHOAS_CATSRC_NAMESPACE=${RHOAS_CATSRC_NAMESPACE:-openshift-marketplace} export RHOAS_CATSRC_NAME=${RHOAS_CATSRC_NAME:-rhoas-operators} export RHOAS_NAMESPACE=${RHOAS_NAMESPACE:-openshift-operators} DOCKER_CFG=$(mktemp) chmod -r $DOCKER_CFG echo "Installing Service Binding Operator" curl -s https://raw.githubusercontent.com/redhat-developer/service-binding-operator/master/install.sh | \ OPERATOR_INDEX_IMAGE=$SBO_INDEX_IMAGE \ OPERATOR_CHANNEL=$SBO_CHANNEL \ OPERATOR_PACKAGE=$SBO_PACKAGE \ CATSRC_NAMESPACE=$SBO_CATSRC_NAMESPACE \ CATSRC_NAME=$SBO_CATSRC_NAME \ SKIP_REGISTRY_LOGIN=true \ DOCKER_CFG=$DOCKER_CFG \ /bin/bash -s rm -f $DOCKER_CFG echo "Installing RHOAS Operator" oc apply -f - &lt;&lt; EOD --- apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: $RHOAS_CATSRC_NAME namespace: $RHOAS_CATSRC_NAMESPACE spec: displayName: RHOAS Operators icon: base64data: "" mediatype: "" image: $RHOAS_INDEX_IMAGE priority: -400 publisher: RHOAS sourceType: grpc updateStrategy: registryPoll: interval: 260s --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: $RHOAS_PACKAGE namespace: $RHOAS_NAMESPACE spec: channel: $RHOAS_CHANNEL installPlanApproval: Automatic name: $RHOAS_PACKAGE source: $RHOAS_CATSRC_NAME sourceNamespace: $RHOAS_CATSRC_NAMESPACE EOD #Wait for the operator to get up and running retries=50 until [[ $retries == 0 ]]; do kubectl get deployment/rhoas-operator -n $RHOAS_NAMESPACE &gt;/dev/null 2&gt;&amp;1 &amp;&amp; break echo "Waiting for rhoas-operator to be created in $RHOAS_NAMESPACE namespace" sleep 5 retries=$(($retries - 1)) done kubectl rollout status -w deployment/rhoas-operator -n $RHOAS_NAMESPACE &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following command to execute the script:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;SBO_INDEX_IMAGE=registry.redhat.io/redhat/redhat-operator-index:v4.7 SBO_CHANNEL=preview SBO_PACKAGE=rh-service-binding-operator ./install-operators.sh&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Simulating active users&lt;/h2&gt; &lt;p&gt;As mentioned in Part 1, the Developer Sandbox setup tool can extend the "activity" of simulated active users in one namespace by adding additional workloads. For the purpose of this test, I used two sets of &lt;em&gt;Deployment + Service + Route&lt;/em&gt; resources, one for the backing service and one for the application. I included a single &lt;code&gt;ServiceBinding&lt;/code&gt; to bind the backing service route URL to the application.&lt;/p&gt; &lt;p&gt;The backing service is a &lt;a href="https://busybox.net/"&gt;BusyBox&lt;/a&gt; container with an exposed route (the URL to be bound to the application). The application is Service Binding Operator's &lt;a href="https://github.com/redhat-developer/service-binding-operator/tree/master/test/acceptance/resources/apps/sbo-generic-test-app" target="_blank"&gt;generic test application&lt;/a&gt;, which is good for testing because it is simple and lightweight.&lt;/p&gt; &lt;p&gt;The simulation tool "provisions" users in sequence. This means that it registers each simulated user into the sandbox. Then, if the user is defined as active, it creates a workload in one of the user's namespaces to simulate their "activity."&lt;/p&gt; &lt;p&gt;To run the simulation tool, I used the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;go run setup/main.go --template=&lt;workload-template-file&gt; --operators-limit 0 --users 2000 --active 2000 --username zippy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I used two slightly different approaches (scenarios) to determine performance. Each scenario is represented by a workload template file referenced in the above command and described in the following sections.&lt;/p&gt; &lt;h3&gt;Scenario 1: With a ServiceBinding resource&lt;/h3&gt; &lt;p&gt;The first scenario ("with SBR") includes a &lt;code&gt;ServiceBinding&lt;/code&gt; resource in the users' provisioning. The backing service and application are created in the active user's namespace together with a &lt;code&gt;ServiceBinding&lt;/code&gt;, so that the Service Binding Operator needs to perform the binding only intermittently. The load on the Service Binding Operator to process &lt;code&gt;ServiceBinding&lt;/code&gt; resources and perform the binding is distributed throughout the duration of the user provisioning, which takes a couple of hours for 2,000 users. The workload template for this scenario is &lt;code&gt;perf-test.with-sbr.yaml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kind: Template apiVersion: v1 metadata: name: sbo-perf-with-sbr objects: - apiVersion: apps/v1 kind: Deployment metadata: name: sbo-perf-app labels: app: sbo-perf-app spec: replicas: 1 strategy: type: RollingUpdate selector: matchLabels: app: sbo-perf-app template: metadata: labels: app: sbo-perf-app spec: containers: - name: sbo-generic-test-app image: quay.io/redhat-developer/sbo-generic-test-app:20200923 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 - apiVersion: v1 kind: Service metadata: labels: app: sbo-perf-app name: sbo-perf-app spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: sbo-perf-app - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: sbo-perf-app name: sbo-perf-app annotations: service.binding/host: path={.spec.host} spec: port: targetPort: 8080 to: kind: "Service" name: sbo-perf-app - apiVersion: apps/v1 kind: Deployment metadata: name: sbo-perf-svc labels: app: sbo-perf-svc spec: replicas: 1 strategy: type: RollingUpdate selector: matchLabels: app: sbo-perf-svc template: metadata: labels: app: sbo-perf-svc spec: containers: - name: busybox image: busybox imagePullPolicy: IfNotPresent command: ['sh', '-c', 'echo Container 1 is Running ; sleep 3600'] ports: - containerPort: 8080 - apiVersion: v1 kind: Service metadata: labels: app: sbo-perf-svc name: sbo-perf-svc spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: sbo-perf-svc - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: sbo-perf-svc name: sbo-perf-svc annotations: service.binding/host: path={.spec.host} spec: port: targetPort: 8080 to: kind: "Service" name: sbo-perf-svc - apiVersion: binding.operators.coreos.com/v1alpha1 kind: ServiceBinding metadata: name: service-binding spec: services: - group: route.openshift.io version: v1 kind: Route name: sbo-perf-svc application: name: sbo-perf-app group: apps version: v1 resource: deployments &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Scenario 2: Without the ServiceBinding resource&lt;/h3&gt; &lt;p&gt;The second scenario ("without SBR") provisions users without the &lt;code&gt;ServiceBinding&lt;/code&gt; resource. The backing service and the application are created in the active user's namespace without a &lt;code&gt;ServiceBinding&lt;/code&gt;. The workload template for this scenario is &lt;code&gt;perf-test.without-sbr.yaml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kind: Template apiVersion: v1 metadata: name: sbo-perf-without-sbr objects: - apiVersion: apps/v1 kind: Deployment metadata: name: sbo-perf-app labels: app: sbo-perf-app spec: replicas: 1 strategy: type: RollingUpdate selector: matchLabels: app: sbo-perf-app template: metadata: labels: app: sbo-perf-app spec: containers: - name: sbo-generic-test-app image: quay.io/redhat-developer/sbo-generic-test-app:20200923 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 - apiVersion: v1 kind: Service metadata: labels: app: sbo-perf-app name: sbo-perf-app spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: sbo-perf-app - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: sbo-perf-app name: sbo-perf-app annotations: service.binding/host: path={.spec.host} spec: port: targetPort: 8080 to: kind: "Service" name: sbo-perf-app - apiVersion: apps/v1 kind: Deployment metadata: name: sbo-perf-svc labels: app: sbo-perf-svc spec: replicas: 1 strategy: type: RollingUpdate selector: matchLabels: app: sbo-perf-svc template: metadata: labels: app: sbo-perf-svc spec: containers: - name: busybox image: busybox imagePullPolicy: IfNotPresent command: ['sh', '-c', 'echo Container 1 is Running ; sleep 3600'] ports: - containerPort: 8080 - apiVersion: v1 kind: Service metadata: labels: app: sbo-perf-svc name: sbo-perf-svc spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: sbo-perf-svc - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: sbo-perf-svc name: sbo-perf-svc annotations: service.binding/host: path={.spec.host} spec: port: targetPort: 8080 to: kind: "Service" name: sbo-perf-svc &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Only after all of the users are provisioned and the resources have settled into place are the &lt;code&gt;ServiceBinding&lt;/code&gt; resources created. All of this happens in a very short time—in fact, almost simultaneously. The following script creates all the &lt;code&gt;ServiceBinding&lt;/code&gt; resources, one for each active users namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get deploy --all-namespaces -o json | jq -rc '.items[] | select(.metadata.name | contains("sbo-perf-app")).metadata.namespace' &gt; workload.namespace.list split -l 300 workload.namespace.list sbr-segment for i in sbr-segment*; do for j in $(cat $i); do oc apply -n $j --server-side=true -f - &lt;&lt; EOD apiVersion: binding.operators.coreos.com/v1alpha1 kind: ServiceBinding metadata: name: service-binding spec: services: - group: route.openshift.io version: v1 kind: Route name: sbo-perf-svc application: name: sbo-perf-app group: apps version: v1 resource: deployments EOD sleep 0.02s done &amp; done wait rm -rf sbr-segment* rm -rf workload.namespace.list &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Creating the &lt;code&gt;ServiceBinding&lt;/code&gt; resources all at once simulates a situation where all of the active users do the binding in their namespace almost simultaneously. This happens, for example, when thousands of Red Hat Summit attendees see an announcement or demonstration at the same time and start playing with the Service Binding Operator in their sandboxes. That's the most extreme stress under which the Service Binding Operator will run, and the test should ensure that nothing crashes.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;In this article you saw how to set up the test environment for testing the Service Binding Operator's real-life performance under stress. In Part 3, we will look at the infrastructure to collect metrics during the tests.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/15/troubleshooting-application-performance-red-hat-openshift-metrics-part-2-test" title="Troubleshooting application performance with Red Hat OpenShift metrics, Part 2: The test environment"&gt;Troubleshooting application performance with Red Hat OpenShift metrics, Part 2: The test environment&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/THpX2R8G2-U" height="1" width="1" alt=""/&gt;</summary><dc:creator>Pavel Macik</dc:creator><dc:date>2021-07-15T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/15/troubleshooting-application-performance-red-hat-openshift-metrics-part-2-test</feedburner:origLink></entry><entry><title type="html">Narayana LRA Update</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/iS0wYf4ZJDI/narayana-lra-update.html" /><author><name>Michael Musgrove</name></author><id>https://jbossts.blogspot.com/2021/07/narayana-lra-update.html</id><updated>2021-07-14T10:28:00Z</updated><content type="html">INTRODUCTION This is another in a series of blogs about the compensation based approach to transactions that the team have been producing over the years. The latest such model is LRA (Long Running Actions), originally based on the 2006 OASIS LRA spec, which was recently been by the Eclipse Foundation. and the are also available. Although LRA is a simple protocol it has a number of interesting features and one blog won’t do it justice. In this, the first one, I will describe how to create a simple microservice that executes in the context of an LRA. Section 1 explains how to create and run an LRA coordinator, section 2 describes how to create and run a participant, and in the final section there is a short review of the many blogs on the subject that the team have created during the past decade. These blogs are an excellent source of wisdom so I will try to avoid repeating old ground and refer the reader to those blogs for the details of the general approach (of which MP-LRA is just the latest incarnation). In follow up blogs the team and I plan to cover, in no particular order: * how to participate in failure recovery (including participant, coordinator and network failures) * writing participants in languages other than Java * nesting LRA’s to structure business flows (into hierarchies) * various methods of triggering the cancellation of an LRA (resulting in the reliable invocation all compensation activities) * leaving LRA’s early * inspecting the progress of participants * inspecting failed participants (i.e. ones which have finished in a failed state) * restarting crashed participants on different endpoints * show services interacting with each other in different JVMs * show services running on OpenShift * leveraging quarkus and WildFly features to simplify the development process (using extensions and galleon feature packs) * addressing the demands that cloud infrastructures, such as OpenShift, place on LRA’s * investigate some best practices and future plans for managing the availability of coordinators and participants (including participant storage, different storage types such as databases and journals, and scaling of coordinators) * strategies for writing compensation logic * and I’m sure my colleagues will have plenty of other topics to add to this list. THE EXAMPLE A Long Running Action is an interaction between microservices such that all parties (called LRA participants) are guaranteed to be notified when the interaction finishes (in either a successful closing state or an unsuccessful cancelling state). A JAX-RS resource participates in an interaction by marking one or more of its methods with the @LRA annotation and by marking another of its methods with an @Compensate annotation. When a method marked with @LRA is invoked the resource is enlisted in the LRA. Enlisting with an LRA means that if the associated LRA is cancelled then the method annotated with @Compensate is invoked reliably (i.e. it will continue to be called until it is definite that the method executed successfully and that the coordinator received the response). The resource may also request that it be reliably notified if the LRA is closed by marking one of its methods with an @Complete annotation. Note that the LRA id is available to all annotated methods so that all parties know which context is Active. In order to implement the guarantees stated in the previous paragraph, the narayana implementation requires that there are one or more LRA coordinators running in the system. A coordinator runs on behalf of many services and is responsible for starting and ending LRA’s and for managing the participant membership in the LRA, in other words it must be available for an interaction to progress (start, enlist and end). Similarly, participant resources must be available during the end phase of the LRA so they too must be restarted if they fail. Note that the developer is normally unconcerned with the coordinator and a typical installation will run them as part of the platform, freeing up the developer to concentrate on the business of creating microservices. However, for the purposes of the blog, first I’ll indicate how you can create one. Note that there is a similar . Later on we will make the blog examples available in the same repo. STARTING A COORDINATOR Here we show how to build and run a REST based coordinator from scratch as a java executable using the quarkus framework. The Narayana LRA coordinator is a JAX-RS resource so it needs the quarkus resteasy extension and it needs to depend on the Narayana LRA coordinator implementation. First generate a quarkus application using the quarkus-maven-plugin, specifying the resteasy-jackson and rest-client extensions which pull in everything we need for JAX-RS: mvn io.quarkus:quarkus-maven-plugin:2.0.1.Final:create \ -DprojectGroupId=org.acme \ -DprojectArtifactId=narayana-lra-coordinator \ -Dextensions="resteasy-jackson,rest-client" cd narayana-lra-coordinator To obtain coordinator support add the org.jboss.narayana.rts:lra-coordinator-jar:5.12.0.Final maven dependency to the dependencies section of the generated pom.xml file as follows: &lt;dependency&gt; &lt;groupId&gt;org.jboss.narayana.rts&lt;/groupId&gt; &lt;artifactId&gt;lra-coordinator-jar&lt;/artifactId&gt; &lt;version&gt;5.12.0.Final&lt;/version&gt; &lt;/dependency&gt; Here I have chosen the latest release (5.12.0.Final) of the Narayana LRA coordinator. Because we just need the quarkus framework for running the coordinator, remove the generated example: rm -rf src. Now build and start the coordinator on port 8080: rm -rf src mvn clean package java -Dquarkus.http.port=8080 -jar target/quarkus-app/quarkus-run.jar &amp;amp; If you want to check that the coordinator is running try listing the active LRA’s: curl http://localhost:8080/lra-coordinator By default the coordinator stores records in the filesystem in a directory called ObjectStore in the user directory (i.e. the value of the system property user.dir). You can change the location by adding a file called src/main/resources/jbossts-properties.xml with content: &lt;!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd"&gt; &lt;properties&gt; &lt;!-- unique id of an LRA coordinator --&gt; &lt;entry key="CoreEnvironmentBean.nodeIdentifier"&gt;1&lt;/entry&gt; &lt;!-- location of the LRA logs --&gt; &lt;entry key="ObjectStoreEnvironmentBean.objectStoreDir"&gt;target/lra-logs&lt;/entry&gt; &lt;!-- location of the communications store --&gt; &lt;entry key="ObjectStoreEnvironmentBean.communicationStore.objectStoreDir"&gt;target/lra-logs&lt;/entry&gt; &lt;/properties&gt; You can test the coordinator is operating correctly by trying to create an LRA using curl, for example. curl -XPOST http://localhost:8080/lra-coordinator/start http://localhost:8080/lra-coordinator/0_ffffc0a8000e_9471_60ed85da_a Note the id of the new LRA in the output. Now try closing the LRA (include the uid part of the LRA id followed by /close): curl -XPUT http://localhost:8080/lra-coordinator/0_ffffc0a8000e_9471_60ed85da_a/close Closed You may verify that the coordinator no longer has a record of the LRA: curl http://localhost:8080/lra-coordinator [] The output will be a json array ([]) of the LRA’s that the coordinator is managing. Check that the array does not contain the id of the LRA that you have just successfully closed. WRITING AND RUNNING AN LRA PARTICIPANT We will generate and run a microservice that participates in an LRA using quarkus. A participant should be a JAX-RS resource so we will use the quarkus-maven-plugin, specifying the resteasy-jackson and rest-client extensions (the reason we need rest-client is that the narayana-lra participant support is implemented via a JAX-RS filter which will intercept business requests and needs to invoke the coordinator via JAX-RS calls): cd .. mvn io.quarkus:quarkus-maven-plugin:2.0.1.Final:create \ -DprojectGroupId=org.acme \ -DprojectArtifactId=narayana-lra-quickstart \ -Dextensions="resteasy-jackson,rest-client" cd narayana-lra-quickstart There is an outstanding pull request for a narayana-lra quarkus extension (io.quarkus:quarkus-narayana-lra) which includes the necessary support for LRA. Since that isn’t available yet you need to manually do what the extension will do (which, fortunately, is neither difficult nor complex): Include the following maven dependencies in the generated pom: &lt;dependency&gt; &lt;groupId&gt;org.eclipse.microprofile.lra&lt;/groupId&gt; &lt;artifactId&gt;microprofile-lra-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.narayana.rts&lt;/groupId&gt; &lt;artifactId&gt;narayana-lra&lt;/artifactId&gt; &lt;version&gt;5.12.0.Final&lt;/version&gt; &lt;/dependency&gt; These two dependencies pull in support for the MicroProfile LRA annotations and the Narayana LRA implementation of the behaviour implied by these annotations. We also need to tell quarkus (via the application.properties config file) to exclude some types from its CDI processing (these types are pulled in by the narayana dependency): echo "quarkus.arc.exclude-types=io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantRegistry,io.narayana.lra.filter.ServerLRAFilter,io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantResource" &gt;&gt; src/main/resources/application.properties And finally, we just need to update the generated Java JAX-RS resource source code to make use of Long Running Actions (which is the most interesting part for developers): Open the file src/main/java/org/acme/GreetingResource.java in an editor and annotate the hello method with an @LRA annotation. In addition add two callback methods which will be called when the LRA is closed or cancelled. // import annotation definitions import org.eclipse.microprofile.lra.annotation.ws.rs.LRA; import org.eclipse.microprofile.lra.annotation.Compensate; import org.eclipse.microprofile.lra.annotation.Complete; // import the definition of the LRA context header import static org.eclipse.microprofile.lra.annotation.ws.rs.LRA.LRA_HTTP_CONTEXT_HEADER; // import some JAX-RS types import javax.ws.rs.PUT; import javax.ws.rs.core.Response; import javax.ws.rs.HeaderParam; ... // annotate the hello method so that it will run in an LRA: @GET @Produces(MediaType.TEXT_PLAIN) @LRA(LRA.Type.REQUIRED) // an LRA will be started before method execution and ended after method execution public String hello(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { return "Hello RESTEasy"; } // ask to be notified if the LRA closes: @PUT // must be PUT @Path("/complete") @Complete public Response completeWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { return Response.ok().build(); } // ask to be notified if the LRA cancels: @PUT // must be PUT @Path("/compensate") @Compensate public Response compensateWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { return Response.ok().build(); } Now build and start the application: mvn clean package -DskipTests java -Dquarkus.http.port=8081 -jar target/quarkus-app/quarkus-run.jar &amp;amp; Ensure that the application and the coordinator are running on different ports, here I use 8081 for the application with the coordinator listening on port 8080 which is the default (I will show in a later blog how to change the default location of the coordinator). Make a REST request to the hello method: curl http://localhost:8081/hello Just before the hello method is invoked an LRA will be started and the participant resource will be enlisted with the LRA. After the method finishes the LRA will be ended automatically (which is the default behaviour of the @LRA annotation). Ending the LRA triggers the termination phase in which the coordinator will invoke the @Complete method (called completeWork in the example) or the @Compensate method (called compensateWork in the example) of each enlisted participant depending on whether the LRA is closing or cancelling. If you want to verify that things are working as expected try updating the resource example to print the value of the HTTP header called Long-Running-Action (see the Java constant LRA_HTTP_CONTEXT_HEADER) which gets injected as a method parameter to each of the annotated methods. Alternatively run the participant in a debugger, for example if you break point inside the hello method and inspect the lraId method parameter and then compare it with what the coordinator knows (curl http://localhost:8080/lra-coordinator) then you should notice that the LRA is in the Active state. Then release the debugger and check back with the coordinator (the LRA will be gone since it should have completed). Note also that the lraId parameter should be the same as the one passed to the @Complete method so setting a break point in that method may also be illuminating. RECAP OF WHAT WE’VE SAID BEFORE ABOUT COMPENSATIONS 12/2017 In this blog Ondra Chaloupka provided an overview of the Saga pattern and then identified those features of LRA that implement the pattern. His article also provided links to the Narayana code and quickstarts, and in particular introduced a worked example of how to run it in a cloud based environment using Minishift (OpenShift on your laptop). 12/2017 Another interesting article contributed by Ondra where he takes a different approach to explaining the concepts and mechanics of LRA’s. In this essay he compares and contrasts the Narayana LRA implementation with two popular Saga implementations: Axon framework and Eventuate.io. This approach is particularly useful for users already familiar with these other frameworks to get a rapid understanding of what LRA is offering. 11/2017 Tom Jenkinson proves "a high-level comparison of the approach taken by the LRA framework with a paper released to the 2017 IEEE 24th International Conference on Web Services - “WSO: Developer-Oriented Transactional Orchestration of Web-Services”." describing the various concepts introduced in both approaches: ordering compensations, idempotency, structure, ease of use, locking and orchestration and nesting of activities. Tom describes LRA thus: "This specification is tailored to addressing needs of applications which are running in highly concurrent environments and have the need to ensure updates to multiple resources have atomic outcomes, but where locking of the resource manager has an unacceptable impact on the overall throughput of the system. LRA has been developed using a cloud first philosophy and achieves its goal by providing an extended transaction model based on Sagas. It provides a set of APIs and components designed to work well in typical microservice architectures." 06/2017 Yet another article provided by Ondra, a busy year for him. Each of Ondra’s 2017 articles, though focused on communicating what LRA is and when, where and why it can be useful, do an excellent job at covering different facets of the compensation based approach to achieving distributed consistency. Ondra provides an extensive overview of the various concepts involved in these two transaction models [sagas and LRA’s]. Of particular interest is the extensive set of references provided at the end of the blog. 10/2016 This article provides an overview of the problem domain that LRA addresses and draws attention to some of the difficulties that naive approaches run into when attempting their resolution. Although the article, written in 2016, predates the Narayana LRA implementation it does outline the basic LRA protocol. 07/2013 COMPENSATING TRANSACTIONS: WHEN ACID IS TOO MUCH An epic four part series contributed by Paul Robinson covering many aspects of the compensation based approach to transactions: . This part will cover situations where you need to coordinate multiple non-transactional resources, such as sending an email or invoking a third party service. : This part covers a scenario where the transaction is distributed, and potentially crosses multiple business domains. . This part covers transactions that span long periods of time and shows how it’s possible to continue the transaction even if some work fails. 05/2015 , 05/2014 AND 04/2015 Three posts in which Mark allays a number of fears, concerns and fallacies that developers may have with combining transactions with microservices. 03/2011 Mark introduces his short post with: "Given that the traditional ACID transaction model is not appropriate for long running/loosely coupled interactions, let’s pose the question, “what type of model or protocol is appropriate?”", and then he goes on to answer the question he poses. Along the way we find definitions and links to papers that define various extensions to the traditional model giving us the "lay of land", so to speak, to enable us to navigate our way to an understanding of alternate models. 03/2011 Another short post in which Mark presents the motivation for long-running activities. 03/2011 Mark motivates the case for REST based transaction protocols as a . LRA is such a REST based protocol and his post provides important background material on why LRA exists alongside WS-BA. 10/2011 Some useful background information on nested transactions (partially motivates nested LRA’s) -------------------------------------------------------------------------------- Last updated 2021-07-14 11:05:48 BST&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/iS0wYf4ZJDI" height="1" width="1" alt=""/&gt;</content><dc:creator>Michael Musgrove</dc:creator><feedburner:origLink>https://jbossts.blogspot.com/2021/07/narayana-lra-update.html</feedburner:origLink></entry><entry><title type="html">This Week in JBoss - 14 July 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/0OSnITCq3G8/weekly-2021-07-14.html" /><category term="quarkus" /><category term="DMN" /><category term="Drools" /><category term="Wildfly" /><category term="transactions" /><category term="narayana" /><category term="kie" /><author><name>Paul Robinson</name><uri>https://www.jboss.org/people/paul-robinson</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-07-14.html</id><updated>2021-07-14T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, DMN, Drools, Wildfly, transactions, narayana, kie"&gt; &lt;h1&gt;This Week in JBoss - 14 July 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Welcome to this edition of the JBoss Editorial bringing you news and updates from the community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_from_the_community"&gt;From the community&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;In &lt;a href="https://jbossts.blogspot.com/2021/07/narayana-lra-update.html"&gt;this post&lt;/a&gt; the Narayana team begins a new blog series on Long Running Actions, a compensation-based approach to transactions.&lt;/p&gt; &lt;p&gt;The KIE Tooling team &lt;a href="https://blog.kie.org/2021/07/instantaneous-feedback-loop-for-dmn-authoring-with-dmn-runner.html"&gt;announce&lt;/a&gt; the new DMN Runner on &lt;a href="http://dmn.new"&gt;dmn/new&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Gabriele Cardosi provides a concrete example of a recommendation engine based on top of PMML in &lt;a href="https://blog.kie.org/2021/07/shopping-recommendations-in-pmml.html"&gt;Shopping recommendations in PMML&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Luca Molteni teaches us &lt;a href="https://blog.kie.org/2021/07/how-to-start-contributing-to-drools-executable-model.html"&gt;How to start contributing to Drools Executable Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Eric Schabell continues his blog series on Cloud Adoption with a &lt;a href="https://www.schabell.org/2021/06/cloud-adoption-example-adoption-architeccture.html"&gt;an Example adoption architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Read this post by Jeff Mesnil to learn how to &lt;a href="https://www.wildfly.org//news/2021/07/01/wildfly-preview-bootable-jar/"&gt;Run WildFly Preview of Jakarta EE 9.1 with Bootable Jar&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_developers_on_film"&gt;Developers on film&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;We don’t just publish blog posts! Here’s some recordings of live community events that happened over the past 2 weeks.&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=n6D6a4G6ozg"&gt;Quarkus Insights #56: Quarkus Configuration&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=e_kwOJE2vQo&amp;#38;"&gt;Quarkus Insights #57: Quarkus CLI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=BIZHbsDAhm4"&gt;KIE Live #38: Destroy wrong beliefs and run real processes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=u3KbGjEkkDM"&gt;KIE Live #39: Migrating Drools to the cloud with Kogito: a step by step path&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases"&gt;Releases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-0-2-final-released/"&gt;Quarkus 2.0.2.Final&lt;/a&gt; The second bug-fix release to the recent, huge &lt;a href="https://quarkus.io/blog/quarkus-2-0-0-final-released/"&gt;Quarkus 2.0 release&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/07/kogito-tooling-0-11-0-released.html"&gt;Kogito Tooling 0.11.0&lt;/a&gt; Several bug fixes and improvements, including the new DMN Runner on &lt;a href="https://dmn.new/"&gt;dmn.new&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://resteasy.github.io/2021/07/08/resteasy-4.7.0.Final/"&gt;RESTEasy 4.7.0.Final&lt;/a&gt;. Including a &lt;a href="https://docs.jboss.org/resteasy/docs/4.7.0.Final/userguide/html/RESTEasy_Embedded_Container.html#d4e2922"&gt;Reactor-Netty server adaptor&lt;/a&gt;, Better support for Java 16 and the up and coming Java 17 release and &lt;a href="https://issues.redhat.com/browse/RESTEASY-2943"&gt;Support for reactive publishers&lt;/a&gt; for the HTTP client.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://www.davsclaus.com/2021/07/apache-camel-311-whats-new.html"&gt;Apache Camel 3.11&lt;/a&gt;. The next long term support release of Camel. See the blog post for what’s new since 3.10.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/paul-robinson.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Paul Robinson&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/0OSnITCq3G8" height="1" width="1" alt=""/&gt;</content><dc:creator>Paul Robinson</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-07-14.html</feedburner:origLink></entry><entry><title>Node.js serverless functions on Red Hat OpenShift, Part 2: Debugging locally</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/dtR3hfYpuvA/nodejs-serverless-functions-red-hat-openshift-part-2-debugging-locally" /><author><name>Lucas Holmquist</name></author><id>cbd5d5fe-b00a-46d3-95e1-123856ec6c40</id><updated>2021-07-13T07:00:00Z</updated><published>2021-07-13T07:00:00Z</published><summary type="html">&lt;p&gt;Welcome back to our series on using serverless functions on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. The previous article introduced you to &lt;a href="https://developers.redhat.com/articles/2021/05/26/nodejs-serverless-functions-openshift-logging"&gt;how logging works in Node.js&lt;/a&gt; and how to customize what is logged in a Node.js function application. Now, we'll take a look at how to debug &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; function-based applications. Because debugging is a longer topic, we'll cover it in two parts. This article walks through how to set up and debug function applications locally with Visual Studio Code (VS Code). The next article will show you how to connect and debug function applications running in a container on a cluster.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: For an introduction to logging function-based applications, see &lt;em&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/26/nodejs-serverless-functions-openshift-logging"&gt;Node.js serverless functions on Red Hat OpenShift, Part 1: Logging&lt;/a&gt;&lt;/em&gt;. For an overview of Red Hat OpenShift Serverless Functions, see &lt;em&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions"&gt;Create your first serverless function with Red Hat OpenShift Serverless Functions&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To follow along with this article, you will need to install Node.js and download the example application from &lt;a href="https://github.com/nodeshift-blog-examples/debugging-with-functions"&gt;GitHub&lt;/a&gt;. We'll also use &lt;a href="https://code.visualstudio.com/"&gt;VS Code&lt;/a&gt; for its easy-to-use built-in debugger.&lt;/p&gt; &lt;p&gt;As with the &lt;a href="https://developers.redhat.com/articles/2021/05/26/nodejs-serverless-functions-openshift-logging" target="_blank"&gt;previous article&lt;/a&gt;, we scaffolded this function application with the &lt;code&gt;kn func&lt;/code&gt; command-line interface (CLI) tool. If you are not already familiar with it, you can learn more by reading &lt;a href="https://developers.redhat.com/blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions"&gt;&lt;em&gt;Create your first serverless function with Red Hat OpenShift Serverless Functions&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Setting up the function application in Visual Studio Code&lt;/h2&gt; &lt;p&gt;Use Git to clone the &lt;a href="https://github.com/nodeshift-blog-examples/debugging-with-functions"&gt;example repository&lt;/a&gt; and then open it up in VS Code. We can see that this Node.js function application is just like any other Node.js application, with an &lt;code&gt;index.js&lt;/code&gt; file where the main function logic is located.&lt;/p&gt; &lt;p&gt;Before we continue, let's put a breakpoint right around line 30, which is inside the &lt;code&gt;invoke&lt;/code&gt; function (see Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Adding a breakpoint to a serverless function in VS Code." data-entity-type="file" data-entity-uuid="4833664b-66aa-4a06-bf6c-dcffd720c020" src="https://developers.redhat.com/sites/default/files/inline-images/functions-debugging-set-break-point.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Adding a breakpoint to a serverless function in VS Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We are setting the breakpoint here because we want to be able to halt the execution of the function when it is called, and the &lt;code&gt;invoke&lt;/code&gt; function is the entry point generated by the &lt;code&gt;kn func&lt;/code&gt; CLI tool. This allows us to step through the code and inspect the different variables the function provides as the function executes.&lt;/p&gt; &lt;p&gt;Let's take a look at the &lt;code&gt;package.json&lt;/code&gt; file. We can see in the following code example that there are three npm scripts generated by the &lt;code&gt;kn func&lt;/code&gt; CLI tool: one for running, another for testing, and another one for debugging. This last script is the one we are interested in.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;"scripts": {     "test": "node test/unit.js &amp;&amp; node test/integration.js",     "local": "faas-js-runtime ./index.js",     "debug": "nodemon --inspect ./node_modules/faas-js-runtime/bin/cli.js ./index.js"   }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are a few things to note about this debug script. First, it uses &lt;a href="https://nodemon.io/"&gt;Nodemon&lt;/a&gt; to start the Node.js process. Nodemon will also detect any code changes and restart the Node.js process when the changes are saved.&lt;/p&gt; &lt;p&gt;The second is the &lt;code&gt;--inspect&lt;/code&gt; flag. This allows us to stop the Node.js process at any breakpoints we set. At the moment, we've only set one.&lt;/p&gt; &lt;p&gt;The last is that the script is called with the &lt;code&gt;faas-js-runtime&lt;/code&gt; CLI. This is a module that provides a Node.js framework for executing a function. The function listens for incoming HTTP requests at &lt;code&gt;localhost:8080&lt;/code&gt;. The incoming request can be a &lt;a href="https://cloudevents.io/"&gt;CloudEvent&lt;/a&gt; or just a simple HTTP GET request. To learn more about the &lt;code&gt;faas-js-runtime&lt;/code&gt;, check out the project on &lt;a href="https://github.com/boson-project/faas-js-runtime"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Debugging the function application&lt;/h2&gt; &lt;p&gt;Starting the debugging process is fairly simple. Select &lt;strong&gt;Start Debugging&lt;/strong&gt; from the &lt;strong&gt;Run&lt;/strong&gt; menu, as shown in Figure 2.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Screenshot showing the debug menu in VS Code." data-entity-type="file" data-entity-uuid="47475d74-466d-499a-a4a2-1d05239e0314" src="https://developers.redhat.com/sites/default/files/inline-images/functions-debugging-start-debugger-run-menu.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Starting the debug process in VS Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This initializes the Node.js process using the &lt;code&gt;--inspect&lt;/code&gt; flag and Nodemon. Once the process starts, your function runs at &lt;a href="http://localhost:8080"&gt;&lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/a&gt;. Navigating to this URL should activate the breakpoint that we set earlier (see Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="breakpoint active" data-entity-type="file" data-entity-uuid="f16f1291-fb1f-4950-bdc6-7b554a17cc21" src="https://developers.redhat.com/sites/default/files/inline-images/functions-debugging-breakpoint-stop-1.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: Activating the breakpoint in VS Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;From here, we can inspect any of the variables that are available to us. Functions are invoked with a &lt;code&gt;context&lt;/code&gt; object, which can be inspected easily using VS Code's variable inspector on the left-hand side of the interface (as shown in Figure 4). This object provides access to the incoming request information. You can get the HTTP request method, any query strings sent with the request, the headers, the HTTP version, or the request body. If the incoming request is a CloudEvent, the CloudEvent itself will also be found on the &lt;code&gt;context&lt;/code&gt; object.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Context object expanded in debug" data-entity-type="file" data-entity-uuid="35ad2c9b-fff2-409b-8102-737083bd5d2b" src="https://developers.redhat.com/sites/default/files/inline-images/debugging-functions-context-var.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4: Context object expanded in debug mode.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The request is a simple GET request. We can see from the variable inspector that it has no body or query params. As with most debugging tools, you can perform many debugging functions like stepping into and over a method, as well as just telling the process to continue executing.&lt;/p&gt; &lt;p&gt;Next, let's send a request to the function with a body. You can use this &lt;code&gt;curl&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -X POST -d '{"hello": "world"}' -H'Content-type: application/json' http://localhost:8080&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When the process stops this time, we can see that there is some data in the &lt;code&gt;context.body&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{   context: {     body: {       hello: “name”      }   } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the request was sent as a &lt;a href="https://cloudevents.io/"&gt;CloudEvent&lt;/a&gt;, this will help you easily inspect the request headers to learn more about it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -X POST -d '{"hello": "world"}' \   -H'Content-type: application/json' \   -H'Ce-id: 1' \   -H'Ce-source: cloud-event-example' \   -H'Ce-type: dev.knative.example' \   -H'Ce-specversion: 0.2' \   http://localhost:8080&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To learn more about this &lt;code&gt;context&lt;/code&gt; object and the parameters it provides to the function developer, &lt;a href="https://openshift-knative.github.io/docs/docs/functions/dev_guide/nodejs/context-obj-reference.html"&gt;check here&lt;/a&gt;. To learn more about CloudEvents, &lt;a href="https://cloudevents.io/"&gt;check here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article introduced you to debugging a Node.js serverless function application locally while you develop the function application. Stay tuned for the next part of this series, where we'll look at how to debug the function application while running inside a container on a Kubernetes cluster such as Red Hat OpenShift.&lt;/p&gt; &lt;p&gt;While you wait, you can read about the latest on &lt;a href="https://openshift-knative.github.io/docs/docs/functions/about-functions.html"&gt;OpenShift Serverless Functions&lt;/a&gt;. To learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/13/nodejs-serverless-functions-red-hat-openshift-part-2-debugging-locally" title="Node.js serverless functions on Red Hat OpenShift, Part 2: Debugging locally"&gt;Node.js serverless functions on Red Hat OpenShift, Part 2: Debugging locally&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/dtR3hfYpuvA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2021-07-13T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/13/nodejs-serverless-functions-red-hat-openshift-part-2-debugging-locally</feedburner:origLink></entry><entry><title type="html">Quarkus 2.0.2.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/wnQ5pYhCBR4/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-2-0-2-final-released/</id><updated>2021-07-13T00:00:00Z</updated><content type="html">We just released Quarkus 2.0.2.Final because we like symmetry (and also because we had a couple of bugfixes to publish). It is a safe upgrade for anyone already using 2.0. If you are not using 2.0 already, please refer to the 2.0 migration guide. Full changelog You can get the...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/wnQ5pYhCBR4" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-2-0-2-final-released/</feedburner:origLink></entry><entry><title type="html">Instantaneous Feedback Loop for DMN Authoring with DMN Runner</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/G6T7nfL6Dyc/instantaneous-feedback-loop-for-dmn-authoring-with-dmn-runner.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/07/instantaneous-feedback-loop-for-dmn-authoring-with-dmn-runner.html</id><updated>2021-07-12T05:00:00Z</updated><content type="html">We’ve been exploring ways to augment the developer experience for our editors on the KIE Tooling team. Today, I’m proud to announce the release of our DMN Runner on . We hope it will transform your DMN Authoring experience. Let’s see it in action: INSTANTANEOUS FEEDBACK LOOP An essential piece of a seamless authoring experience is an instantaneous feedback loop. A feedback loop on asset authoring happens when some portion of or all asset input is captured, analyzed, and used to provide insight to improve a future users’ authoring experience. Providing such instantaneous feedback is crucial to a fluid authoring experience. But what does this mean for DMN authoring? AUTOMATIC FORM GENERATION On every change of your DMN model, the DMN runner introspects it and automatically generates the input form for your DMN execution. The data that you input on it will be later used on the execution loop. AUTOMATIC DMN EVALUATION As soon as the DMN runner is connected to your dmn.new session, on every change of your DMN model, we will combine your DMN model and your form input and evaluate it on the DMN Engine. The awesome part of this workflow is that it is really fast, looking almost instantaneous. See it in action: AUTOMATIC DMN VALIDATION Another cool feature of DMN Runner is that together with DMN evaluation, we also validate your DMN models. Take a look at it: HOW TO START TO USE IT? It’s super simple. Go to and click on the ‘DMN Runner’ button and follow our wizard. The DMN runner is part of the KIE Tooling Extended Services Desktop application, and we provide binaries for all the platforms. After installing, dmn.new will automatically connect to your local environment and augment it, providing a much more fluid experience for your authoring! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/G6T7nfL6Dyc" height="1" width="1" alt=""/&gt;</content><dc:creator>Eder Ignatowicz</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/instantaneous-feedback-loop-for-dmn-authoring-with-dmn-runner.html</feedburner:origLink></entry><entry><title type="html">Kogito Tooling 0.11.0 Released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/WznBW6kzHyI/kogito-tooling-0-11-0-released.html" /><author><name>Eder Ignatowicz</name></author><id>http://feeds.athico.com/~r/droolsatom/~3/FE87raV3XMI/kogito-tooling-0-11-0-released.html</id><updated>2021-07-09T05:00:00Z</updated><content type="html">We have just launched a fresh new Kogito Tooling release! &#x1f389; On the 0.11.0 , we made a lot of improvements and bug fixes. We are also happy to announce that this release marks the first release of our DMN Runner on ! This post will give a quick overview of this . I hope you enjoy it! INSTANTANEOUS FEEDBACK LOOP FOR DMN AUTHORING WITH DMN RUNNER We’ve been exploring ways to augment the developer experience for our editors on the KIE Tooling team. Today, I’m proud to announce the release of our DMN Runner on dmn.new. We hope it will transform your DMN Authoring experience. Let’s see it in action: For this reason, we decided to start a new development stream for our BPMN, DMN, and Scenario Simulator editors called , freeing the way for Editors to continue evolving on both streams separately (Kogito and BC) without carrying the weight of the other. Soon we will write a blog post describing this new awesome feature. NEW FEATURES, FIXED ISSUES, AND IMPROVEMENTS We also made some new features, a lot of refactorings and improvements, with highlights to: * – Online Editor DMN Runner (First Iteration) * – The nodes should be created on top of the selected node in DMN editor * – BPMN Editor – Improve SVG generated ids * – [DMN Designer] Read-only mode – Connectors appear differently on read-only mode * – Implement integration tests for Save operation in BPMN editor in VSCode * – [SceSim Designer] HiDPI is not working as expected * – [DMN/BPMN] Sync kogito-editors-java with latest translations * – Kogito Tooling VS Code extensions Workspaces Trust * – Prevent different envelopes in the same window to interact with each other * – New elements should always be connected by their central magnetic point * – Stunner – Palette fixes &amp;amp; improvements * – Guided tour for invalid DMN models * – It’s not possible to save arrow edits * – SceSim Editor does not work in Eclipse Theia * – [BC included] [DMN/BPMN editor] Sometimes clicking outside doesn’t unselect nodes * – Stunner – Texts overlap toolboxes * – [BC Included] DMN editor removing edges for duplicate Decision Nodes on canvas * – Inconsistent results of integration tests across CI * – Stunner – Unknown Custom tasks in Designer makes Diagram Explorer empty * – Clear selection button doesn’t work on Testing Tools when use click property first time. * – Stunner – Editing text using Inline editor is shown over Properties panel or Expanded Palette * – Stunner – The order of Custom tasks in the palette is different with every process opening FURTHER READING/WATCHING We had some excellent blog posts on Kie Blog that I recommend you read: * , by Kirill Gaevskii; * , by Guilherme Carreiro; * , by Valentino Pellegrino; We also presented in some Kie Lives: * , by William Siqueira; * , by Guilherme Carreiro; * , by Guilherme Caponetto. THANK YOU TO EVERYONE INVOLVED! I would like to thank everyone involved with this release, from the excellent KIE Tooling Engineers to the lifesavers QEs and the UX people that help us look awesome! [kie] The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/WznBW6kzHyI" height="1" width="1" alt=""/&gt;</content><dc:creator>Eder Ignatowicz</dc:creator><feedburner:origLink>http://feeds.athico.com/~r/droolsatom/~3/FE87raV3XMI/kogito-tooling-0-11-0-released.html</feedburner:origLink></entry></feed>
